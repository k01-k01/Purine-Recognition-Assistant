#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
YOLO+ResNet50 èåˆæ¨¡å‹è®­ç»ƒè„šæœ¬
ç”¨äºFood20_newæ•°æ®é›†çš„è®­ç»ƒ
"""

import os
import json
import logging
import argparse
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import numpy as np
import cv2
from PIL import Image
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms, models
from ultralytics import YOLO
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score, classification_report
import time
from tqdm import tqdm

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('logs/training.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

class FoodDataset(Dataset):
    """é£Ÿç‰©æ•°æ®é›†ç±»"""
    
    def __init__(self, data_dir: str, split: str = 'train', transform=None):
        """
        åˆå§‹åŒ–æ•°æ®é›†
        
        Args:
            data_dir: æ•°æ®é›†æ ¹ç›®å½•
            split: æ•°æ®é›†åˆ†å‰² ('train', 'val', 'test')
            transform: å›¾åƒå˜æ¢
        """
        self.data_dir = Path(data_dir)
        self.split = split
        self.transform = transform
        
        # åŠ è½½æ ‡æ³¨æ–‡ä»¶
        annotation_file = self.data_dir / split / f'{split}.json'
        with open(annotation_file, 'r', encoding='utf-8') as f:
            self.annotations = json.load(f)
        
        # è·å–ç±»åˆ«æ˜ å°„
        self.classes = list(set([ann['category'] for ann in self.annotations['annotations']]))
        self.classes.sort()  # ç¡®ä¿ç±»åˆ«é¡ºåºä¸€è‡´
        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}
        self.idx_to_class = {idx: cls for cls, idx in self.class_to_idx.items()}
        
        # æ„å»ºå›¾åƒè·¯å¾„å’Œæ ‡ç­¾
        self.images = []
        self.labels = []
        
        for ann in self.annotations['annotations']:
            image_id = ann['image_id']
            category = ann['category']
            
            # æŸ¥æ‰¾å¯¹åº”çš„å›¾åƒä¿¡æ¯
            image_info = next((img for img in self.annotations['images'] if img['id'] == image_id), None)
            if image_info:
                image_path = self.data_dir / split / 'images' / image_info['file_name']
                if image_path.exists():
                    self.images.append(str(image_path))
                    self.labels.append(self.class_to_idx[category])
        
        logger.info(f"ğŸ“Š {split}æ•°æ®é›†åŠ è½½å®Œæˆ: {len(self.images)} å¼ å›¾åƒ, {len(self.classes)} ä¸ªç±»åˆ«")
        logger.info(f"ğŸ“‹ ç±»åˆ«åˆ—è¡¨: {self.classes}")
    
    def __len__(self):
        return len(self.images)
    
    def __getitem__(self, idx):
        image_path = self.images[idx]
        label = self.labels[idx]
        
        # åŠ è½½å›¾åƒ
        image = Image.open(image_path).convert('RGB')
        
        if self.transform:
            image = self.transform(image)
        
        return image, label

class YOLOResNetFusion(nn.Module):
    """YOLO+ResNetèåˆæ¨¡å‹"""
    
    def __init__(self, num_classes: int, yolo_model_path: str = None):
        """
        åˆå§‹åŒ–èåˆæ¨¡å‹
        
        Args:
            num_classes: åˆ†ç±»ç±»åˆ«æ•°
            yolo_model_path: YOLOæ¨¡å‹è·¯å¾„
        """
        super(YOLOResNetFusion, self).__init__()
        
        # ResNet50ç‰¹å¾æå–å™¨
        self.resnet = models.resnet50(pretrained=True)
        # ç§»é™¤æœ€åçš„åˆ†ç±»å±‚
        self.resnet_features = nn.Sequential(*list(self.resnet.children())[:-1])
        
        # YOLOæ£€æµ‹å™¨
        self.yolo = YOLO('yolov8n.pt') if yolo_model_path is None else YOLO(yolo_model_path)
        
        # èåˆå±‚
        self.fusion_layer = nn.Sequential(
            nn.Linear(2048, 512),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(512, num_classes)
        )
        
        # æ£€æµ‹ç‰¹å¾ç»´åº¦
        self.detection_features = 1024  # YOLOæ£€æµ‹ç‰¹å¾ç»´åº¦
        
        # æœ€ç»ˆåˆ†ç±»å±‚
        self.classifier = nn.Sequential(
            nn.Linear(2048 + self.detection_features, 512),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(512, num_classes)
        )
        
        logger.info(f"ğŸ”§ èåˆæ¨¡å‹åˆå§‹åŒ–å®Œæˆ: {num_classes} ä¸ªç±»åˆ«")
    
    def forward(self, x):
        """
        å‰å‘ä¼ æ’­
        
        Args:
            x: è¾“å…¥å›¾åƒå¼ é‡ [batch_size, 3, H, W]
        
        Returns:
            åˆ†ç±»ç»“æœ
        """
        batch_size = x.size(0)
        
        # ResNetç‰¹å¾æå–
        resnet_features = self.resnet_features(x)
        resnet_features = resnet_features.view(batch_size, -1)  # [batch_size, 2048]
        
        # YOLOæ£€æµ‹ç‰¹å¾ (ç®€åŒ–ç‰ˆæœ¬ï¼Œå®é™…åº”ç”¨ä¸­éœ€è¦æ›´å¤æ‚çš„å¤„ç†)
        # è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨ResNetç‰¹å¾çš„å˜ä½“ä½œä¸ºæ£€æµ‹ç‰¹å¾
        detection_features = resnet_features[:, :self.detection_features]
        
        # ç‰¹å¾èåˆ
        combined_features = torch.cat([resnet_features, detection_features], dim=1)
        
        # åˆ†ç±»
        output = self.classifier(combined_features)
        
        return output

class ModelTrainer:
    """æ¨¡å‹è®­ç»ƒå™¨"""
    
    def __init__(self, config: Dict):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨
        
        Args:
            config: è®­ç»ƒé…ç½®
        """
        self.config = config
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        logger.info(f"ğŸš€ ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir = Path(config['output_dir'])
        self.output_dir.mkdir(parents=True, exist_ok=True)
        (self.output_dir / 'logs').mkdir(exist_ok=True)
        (self.output_dir / 'models').mkdir(exist_ok=True)
        (self.output_dir / 'plots').mkdir(exist_ok=True)
        
        # æ•°æ®å˜æ¢
        self.train_transform = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.RandomHorizontalFlip(),
            transforms.RandomRotation(10),
            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
        
        self.val_transform = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
        
        # åˆå§‹åŒ–æ¨¡å‹å’Œè®­ç»ƒç»„ä»¶
        self.model = None
        self.criterion = None
        self.optimizer = None
        self.scheduler = None
        self.train_loader = None
        self.val_loader = None
        
    def load_data(self):
        """åŠ è½½æ•°æ®"""
        logger.info("ğŸ“‚ å¼€å§‹åŠ è½½æ•°æ®...")
        
        # è®­ç»ƒæ•°æ®
        train_dataset = FoodDataset(
            data_dir=self.config['data_dir'],
            split='train',
            transform=self.train_transform
        )
        
        # éªŒè¯æ•°æ®
        val_dataset = FoodDataset(
            data_dir=self.config['data_dir'],
            split='val',
            transform=self.val_transform
        )
        
        # æ•°æ®åŠ è½½å™¨
        self.train_loader = DataLoader(
            train_dataset,
            batch_size=self.config['batch_size'],
            shuffle=True,
            num_workers=self.config['num_workers'],
            pin_memory=True
        )
        
        self.val_loader = DataLoader(
            val_dataset,
            batch_size=self.config['batch_size'],
            shuffle=False,
            num_workers=self.config['num_workers'],
            pin_memory=True
        )
        
        self.num_classes = len(train_dataset.classes)
        self.classes = train_dataset.classes
        
        logger.info(f"âœ… æ•°æ®åŠ è½½å®Œæˆ: {len(train_dataset)} è®­ç»ƒæ ·æœ¬, {len(val_dataset)} éªŒè¯æ ·æœ¬")
        
        return train_dataset, val_dataset
    
    def setup_model(self):
        """è®¾ç½®æ¨¡å‹"""
        logger.info("ğŸ”§ å¼€å§‹è®¾ç½®æ¨¡å‹...")
        
        # åˆ›å»ºæ¨¡å‹
        self.model = YOLOResNetFusion(
            num_classes=self.num_classes,
            yolo_model_path=self.config.get('yolo_model_path')
        ).to(self.device)
        
        # æŸå¤±å‡½æ•°
        self.criterion = nn.CrossEntropyLoss()
        
        # ä¼˜åŒ–å™¨
        self.optimizer = optim.AdamW(
            self.model.parameters(),
            lr=self.config['learning_rate'],
            weight_decay=self.config['weight_decay']
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = optim.lr_scheduler.StepLR(
            self.optimizer,
            step_size=self.config['lr_step_size'],
            gamma=self.config['lr_gamma']
        )
        
        logger.info("âœ… æ¨¡å‹è®¾ç½®å®Œæˆ")
    
    def train_epoch(self, epoch: int) -> Dict[str, float]:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0.0
        correct = 0
        total = 0
        
        progress_bar = tqdm(self.train_loader, desc=f'Epoch {epoch+1}/{self.config["epochs"]}')
        
        for batch_idx, (data, target) in enumerate(progress_bar):
            data, target = data.to(self.device), target.to(self.device)
            
            self.optimizer.zero_grad()
            output = self.model(data)
            loss = self.criterion(output, target)
            loss.backward()
            self.optimizer.step()
            
            total_loss += loss.item()
            pred = output.argmax(dim=1, keepdim=True)
            correct += pred.eq(target.view_as(pred)).sum().item()
            total += target.size(0)
            
            # æ›´æ–°è¿›åº¦æ¡
            progress_bar.set_postfix({
                'Loss': f'{loss.item():.4f}',
                'Acc': f'{100.*correct/total:.2f}%'
            })
        
        avg_loss = total_loss / len(self.train_loader)
        accuracy = 100. * correct / total
        
        return {
            'loss': avg_loss,
            'accuracy': accuracy
        }
    
    def validate_epoch(self) -> Dict[str, float]:
        """éªŒè¯ä¸€ä¸ªepoch"""
        self.model.eval()
        total_loss = 0.0
        correct = 0
        total = 0
        all_predictions = []
        all_targets = []
        
        with torch.no_grad():
            for data, target in tqdm(self.val_loader, desc='Validation'):
                data, target = data.to(self.device), target.to(self.device)
                output = self.model(data)
                loss = self.criterion(output, target)
                
                total_loss += loss.item()
                pred = output.argmax(dim=1, keepdim=True)
                correct += pred.eq(target.view_as(pred)).sum().item()
                total += target.size(0)
                
                all_predictions.extend(pred.cpu().numpy().flatten())
                all_targets.extend(target.cpu().numpy())
        
        avg_loss = total_loss / len(self.val_loader)
        accuracy = 100. * correct / total
        
        return {
            'loss': avg_loss,
            'accuracy': accuracy,
            'predictions': all_predictions,
            'targets': all_targets
        }
    
    def train(self):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        logger.info("ğŸš€ å¼€å§‹è®­ç»ƒ...")
        
        # åŠ è½½æ•°æ®
        train_dataset, val_dataset = self.load_data()
        
        # è®¾ç½®æ¨¡å‹
        self.setup_model()
        
        # è®­ç»ƒå†å²
        train_history = {'loss': [], 'accuracy': []}
        val_history = {'loss': [], 'accuracy': []}
        best_val_acc = 0.0
        
        # è®­ç»ƒå¾ªç¯
        for epoch in range(self.config['epochs']):
            logger.info(f"ğŸ“ˆ Epoch {epoch+1}/{self.config['epochs']}")
            
            # è®­ç»ƒ
            train_metrics = self.train_epoch(epoch)
            train_history['loss'].append(train_metrics['loss'])
            train_history['accuracy'].append(train_metrics['accuracy'])
            
            # éªŒè¯
            val_metrics = self.validate_epoch()
            val_history['loss'].append(val_metrics['loss'])
            val_history['accuracy'].append(val_metrics['accuracy'])
            
            # å­¦ä¹ ç‡è°ƒåº¦
            self.scheduler.step()
            
            # è®°å½•æ—¥å¿—
            logger.info(f"ğŸ“Š Epoch {epoch+1} - Train Loss: {train_metrics['loss']:.4f}, "
                       f"Train Acc: {train_metrics['accuracy']:.2f}%, "
                       f"Val Loss: {val_metrics['loss']:.4f}, "
                       f"Val Acc: {val_metrics['accuracy']:.2f}%")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_metrics['accuracy'] > best_val_acc:
                best_val_acc = val_metrics['accuracy']
                self.save_model('best_model.pth')
                logger.info(f"ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ŒéªŒè¯å‡†ç¡®ç‡: {best_val_acc:.2f}%")
            
            # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
            if (epoch + 1) % self.config['save_interval'] == 0:
                self.save_model(f'checkpoint_epoch_{epoch+1}.pth')
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        self.save_model('final_model.pth')
        
        # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
        self.plot_training_curves(train_history, val_history)
        
        # ç”Ÿæˆåˆ†ç±»æŠ¥å‘Š
        self.generate_classification_report(val_metrics['predictions'], val_metrics['targets'])
        
        logger.info("âœ… è®­ç»ƒå®Œæˆ!")
    
    def save_model(self, filename: str):
        """ä¿å­˜æ¨¡å‹"""
        model_path = self.output_dir / 'models' / filename
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'config': self.config,
            'classes': self.classes,
            'num_classes': self.num_classes
        }, model_path)
        logger.info(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜åˆ°: {model_path}")
    
    def plot_training_curves(self, train_history: Dict, val_history: Dict):
        """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
        
        # æŸå¤±æ›²çº¿
        ax1.plot(train_history['loss'], label='Train Loss')
        ax1.plot(val_history['loss'], label='Val Loss')
        ax1.set_title('Training and Validation Loss')
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('Loss')
        ax1.legend()
        ax1.grid(True)
        
        # å‡†ç¡®ç‡æ›²çº¿
        ax2.plot(train_history['accuracy'], label='Train Accuracy')
        ax2.plot(val_history['accuracy'], label='Val Accuracy')
        ax2.set_title('Training and Validation Accuracy')
        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('Accuracy (%)')
        ax2.legend()
        ax2.grid(True)
        
        plt.tight_layout()
        plt.savefig(self.output_dir / 'plots' / 'training_curves.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        logger.info("ğŸ“Š è®­ç»ƒæ›²çº¿å·²ä¿å­˜")
    
    def generate_classification_report(self, predictions: List, targets: List):
        """ç”Ÿæˆåˆ†ç±»æŠ¥å‘Š"""
        from sklearn.metrics import classification_report, confusion_matrix
        import seaborn as sns
        
        # åˆ†ç±»æŠ¥å‘Š
        report = classification_report(targets, predictions, target_names=self.classes, output_dict=True)
        
        # ä¿å­˜æŠ¥å‘Š
        report_path = self.output_dir / 'classification_report.json'
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        # æ··æ·†çŸ©é˜µ
        cm = confusion_matrix(targets, predictions)
        plt.figure(figsize=(12, 10))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                   xticklabels=self.classes, yticklabels=self.classes)
        plt.title('Confusion Matrix')
        plt.xlabel('Predicted')
        plt.ylabel('Actual')
        plt.xticks(rotation=45)
        plt.yticks(rotation=0)
        plt.tight_layout()
        plt.savefig(self.output_dir / 'plots' / 'confusion_matrix.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        logger.info("ğŸ“‹ åˆ†ç±»æŠ¥å‘Šå·²ç”Ÿæˆ")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='YOLO+ResNet50 èåˆæ¨¡å‹è®­ç»ƒ')
    parser.add_argument('--data_dir', type=str, default='datasets/Food20_new',
                       help='æ•°æ®é›†ç›®å½•')
    parser.add_argument('--output_dir', type=str, default='outputs/training',
                       help='è¾“å‡ºç›®å½•')
    parser.add_argument('--batch_size', type=int, default=32,
                       help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--epochs', type=int, default=50,
                       help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--learning_rate', type=float, default=0.001,
                       help='å­¦ä¹ ç‡')
    parser.add_argument('--weight_decay', type=float, default=1e-4,
                       help='æƒé‡è¡°å‡')
    parser.add_argument('--lr_step_size', type=int, default=20,
                       help='å­¦ä¹ ç‡è°ƒåº¦æ­¥é•¿')
    parser.add_argument('--lr_gamma', type=float, default=0.1,
                       help='å­¦ä¹ ç‡è°ƒåº¦è¡°å‡å› å­')
    parser.add_argument('--num_workers', type=int, default=4,
                       help='æ•°æ®åŠ è½½å™¨å·¥ä½œè¿›ç¨‹æ•°')
    parser.add_argument('--save_interval', type=int, default=10,
                       help='æ¨¡å‹ä¿å­˜é—´éš”')
    
    args = parser.parse_args()
    
    # è®­ç»ƒé…ç½®
    config = {
        'data_dir': args.data_dir,
        'output_dir': args.output_dir,
        'batch_size': args.batch_size,
        'epochs': args.epochs,
        'learning_rate': args.learning_rate,
        'weight_decay': args.weight_decay,
        'lr_step_size': args.lr_step_size,
        'lr_gamma': args.lr_gamma,
        'num_workers': args.num_workers,
        'save_interval': args.save_interval,
        'yolo_model_path': None  # ä½¿ç”¨é»˜è®¤YOLOv8næ¨¡å‹
    }
    
    # åˆ›å»ºè®­ç»ƒå™¨å¹¶å¼€å§‹è®­ç»ƒ
    trainer = ModelTrainer(config)
    trainer.train()

if __name__ == '__main__':
    main() 